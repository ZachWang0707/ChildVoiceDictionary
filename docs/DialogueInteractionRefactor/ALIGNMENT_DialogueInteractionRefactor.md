# ALIGNMENT - 对话交互逻辑重构

## 项目上下文分析

### 现有项目状态
- 项目名称：ChildVoiceDictionary
- 当前状态：ASR、LLM、TTS 都已调通，基本 MVP 功能可用
- 现有架构：分两次 LLM 调用（先做意图识别，再做释义生成）
- 本次目标：简化为单次 LLM 调用，使用 sys_prompt.txt 统一驱动

### 项目特性规范（保持不变）
- 面向用户：7岁中国儿童
- 交互方式：语音为主，视觉为辅
- 核心功能：英语单词查询、语音识别、语音合成、生词表管理
- 技术栈：React + Vite（前端）+ Node.js（后端）

---

## 原始需求（本次变更）

### 核心变更点

1. **简化 LLM 调用**
   - ❌ 不要：先对话一次做意图识别，再对话一次做回应生成
   - ✅ 要：用户每次发言后，对 LLM 做单次调用

2. **系统提示词处理**
   - ✅ 将 `sys_prompt.txt` 的内容作为 `system` 角色传入
   - ❌ 不要把系统提示词混入对话历史

3. **完整上下文传入**
   - 每次调用时，将完整的对话历史一起传入（user/assistant 交替的消息数组）
   - 历史过长时，保留最近 15 轮，更早的裁剪掉

4. **用 action 字段驱动 UI**
   - LLM 每次返回包含 `action`、`word`、`speech` 的 JSON
   - 应用根据 `action` 的值决定下一步行为（保存单词、播报等）

5. **健壮的 JSON 解析**
   - LLM 返回内容在解析前先清洗掉可能存在的 markdown 代码块标记
   - 解析失败时不要崩溃，兜底处理：
     - 将返回内容直接作为 `speech` 播报
     - `action` 置为 `chat`

### sys_prompt.txt 关键内容摘要

- 角色：陪伴7岁中国孩子学英语的语音助手，名字叫"小词"
- 核心任务：帮孩子查英语单词的意思，记录生词表
- 理解意图原则：自由理解，不要死板匹配固定句式
- 区分读音输入和拼写输入：
  - 读音输入：必须先确认拼写
  - 拼写输入：直接查词，不需要确认
- 孩子中断时：跟着孩子的节奏走，不要追问之前的问题
- 释义规则：只用中文，不用英文句子，控制在 2-3 句以内
- 返回格式：
  ```json
  {
    "action": "confirm_word | explain_word | ask_spell | ask_save | save_word | skip_save | chat",
    "word": "当前处理的单词（如有）",
    "speech": "朗读给孩子听的文字"
  }
  ```

---

## 边界确认

### 本次任务范围
✅ 包含：
- 修改后端 LLM 接口，使用 `sys_prompt.txt` 作为 system 提示词
- 合并两次 LLM 调用为单次调用
- 实现对话历史管理（最多保留 15 轮）
- 实现 JSON 清洗和健壮解析
- 修改前端逻辑，根据 `action` 字段驱动 UI 行为
- **UI 样式更新（新）**：按照新的 UI 设计要求实现
- 保持现有 ASR、TTS、生词表功能不变

❌ 不包含（本次）：
- 新功能开发
- 数据库接入

---

## UI 设计要求（新增）

### UI 原则
尽可能少用眼，界面上就一个大大的按钮，按住发言。其他的东西能省就省。

### 页面布局
1. **右上角按钮**：生词表，搭配个线框风格的图标
2. **页面中间靠上**：六个字，"语音英汉词典"
3. **页面中间**：一个大大的圆形发言按钮，上面有个扁平化的麦克风图案
4. **按钮下方空白处**：debug 信息展示区域，debug mode 打开时显示，关闭时不显示

### 圆形发言按钮的颜色变化逻辑
| 状态 | 按钮颜色 | 说明 |
|------|---------|------|
| 等待用户输入 | 蓝色 | 空闲状态 |
| 按住后、系统准备好录音之前 + 准备好的前 0.5 秒 | 灰色 | 防止用户一按就说话漏录开头 |
| 准备好录音后 + 0.5 秒 | 绿色 | 可以开始说话 |
| 松手后、系统处理中、等待服务端响应 | 灰色 | 处理中 |
| 开始播报 + 0.5 秒后 | 蓝色 | 可以打断发言 |

### 用户按住按钮发言与系统播报的关系
- **只要用户在蓝色状态下按住按钮，立即中断播报**
- **不要做成**：按一下按钮，播报停止，再按住按钮才能开口说话
- **要做成**：按住按钮，立即停止播报，且变绿后就能开口说话

### 重要提示
- LLM 返回的 action 只干预系统行为，**不干预按钮颜色**

---

## 需求理解

### 对现有架构的理解
当前架构是：
1. 调用 `/api/llm/understand` → 识别意图
2. 根据意图决定下一步
3. 需要释义时调用 `/api/llm/explain` → 生成释义

新架构是：
1. 单次调用 `/api/llm/chat`（新接口）
2. 传入完整对话历史 + system 提示词
3. 直接得到 `action`、`word`、`speech`
4. 根据 `action` 执行相应行为

### action 字段理解
可能的 action 值：
- `confirm_word`：确认单词拼写
- `explain_word`：给出单词释义
- `ask_spell`：请孩子拼写单词
- `ask_save`：询问是否加入生词表
- `save_word`：加入生词表
- `skip_save`：不加入生词表
- `chat`：普通聊天

---

## 疑问澄清

### 需要确认的问题

1. **当前单词状态管理**
   - 问题：LLM 返回 `word` 字段时，前端需要保存这个单词吗？
   - 或者：完全依赖 LLM 每次返回的 `word` 字段？

2. **生词表保存的触发时机**
   - 问题：当 `action` 为 `save_word` 时，是直接保存吗？
   - 还是：当 `action` 为 `ask_save` 时，等孩子回答后再保存？

3. **对话历史的存储位置**
   - 问题：对话历史是存在前端 localStorage？
   - 还是：每次都从前端传给后端？

4. **sys_prompt.txt 的位置**
   - 问题：`sys_prompt.txt` 放在项目根目录，后端如何读取？
   - 需要把它移到某个特定目录吗？
